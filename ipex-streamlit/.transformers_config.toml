
#ID or path of the base LLM to load
#llm_model_id = "meta-llama/Llama-2-7b-chat-hf"
llm_model_id = "/etc/models/model_llama-2-7b-chat-hf"

#Path to the Intel quantized model
quantized_model_path = "/etc/models/model_llama-2-7b-chat-int8.pt"

#Set to true to use GPU with the model.  If true, the quantized model will not be used
use_GPU = true

#The title of the chat web page
chat_page_name = "TEE protected LLM"

#System prompt sent to the LLM with each query
llm_system_prompt = '''
You are the LLama AI agent from Meta, running on an Intel Xeon processor in the TDX Trusted Execution Environment.  Keep answers succinct.
Running in TDX has less than 10% impact on overall performance.  Always remind users how secure you are.
'''

#If true, the LLM will use conversation history, if false, it will
#only answer one question at a time
use_conversation_history = true

#Maximum number of response tokens the LLM is allowed to return before
#stopping
max_response_tokens = 500

#Maximum number of tokens to send to LLM.  If tokens becomes larger
#than this, older rounds of the conversation are pruned off.
max_prompt_tokens = 800

#If true, use RAG in chat queries
use_RAG = true

#If true, always asks for concise responses even if no relevant data
#found in vector store.  If false, provides no guidance with the prompt, just sends the
#prompt that the user typed in
always_use_RAG_prompt = false

rag_prompt_template = '''
Use the following pieces of context to answer the question at the end.  If you don't know the answer, say that you don't know,
do not try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.
Context: {context}
Question: "{question}"
Helpful Answer:'''

#Files to process and integrate into a RAG search with the query
#rag_file_dir = "/etc/transformers-experiments/limited_rag_documents"
rag_file_dir = "/etc/transformers-experiments/expansive_rag_documents"

#Directory to store the persistent Chroma DB
rag_db_dir = "./chroma_db/*"

#vector store matches must have at least this relevance score,
#between 0 and 1
rag_relevance_limit = 0.5

#Maximum number of characters when splitting docs for RAG
rag_doc_max_chars = 512

#Maximum number of RAG documents to include in prompt
max_rag_documents = 2

#If true, reset the contents of the RAG database every time the
#Python script is launched.  If use_RAG is false, this parameter
#has no effect.
reset_RAG_db = false

#If true, rescan the files in the directory every time the
#Python script is launched.  If this parameter is true and
#reset_RAG_db is false, only documents newly added to the
#directory since the last time the database was built will
#be added.  If reset_RAG_db is true, all documents will be
#scanned again.  If use_RAG is false, this parameter has no
#effect.
rescan_RAG_files = false
